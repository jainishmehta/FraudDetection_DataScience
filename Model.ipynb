{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28837bfd",
   "metadata": {},
   "source": [
    "# Building Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64d6837",
   "metadata": {},
   "source": [
    "First, do the necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53ca2009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# make predictions\n",
    "from pandas import read_csv\n",
    "from sklearn.utils import resample\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5af7be",
   "metadata": {},
   "source": [
    "Secondly, read the .csv files and examine the categories, particularily for the target column and if the columns are categorical or numerical. At this stage, one should do some exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90526d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>USERID</th>\n",
       "      <th>EVENTID</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  USERID  EVENTID  TIMESTAMP  LABEL\n",
       "0   0       0        0          0      0\n",
       "1   1       0        1          6      0\n",
       "2   2       0        2         41      0\n",
       "3   3       0        1         49      0\n",
       "4   4       0        2         51      0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read the csvs\n",
    "train=pd.read_csv('train.csv')\n",
    "test=pd.read_csv('test.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3cd38da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>USERID</th>\n",
       "      <th>EVENTID</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>391749</td>\n",
       "      <td>6751</td>\n",
       "      <td>8</td>\n",
       "      <td>2497257</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>391750</td>\n",
       "      <td>5059</td>\n",
       "      <td>26</td>\n",
       "      <td>2497260</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>391752</td>\n",
       "      <td>2816</td>\n",
       "      <td>49</td>\n",
       "      <td>2497261</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>391752</td>\n",
       "      <td>2816</td>\n",
       "      <td>49</td>\n",
       "      <td>2497261</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>391753</td>\n",
       "      <td>6751</td>\n",
       "      <td>7</td>\n",
       "      <td>2497261</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID  USERID  EVENTID  TIMESTAMP  LABEL\n",
       "0  391749    6751        8    2497257    NaN\n",
       "1  391750    5059       26    2497260    NaN\n",
       "2  391752    2816       49    2497261    NaN\n",
       "3  391752    2816       49    2497261    NaN\n",
       "4  391753    6751        7    2497261    NaN"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19139827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels with 0:387850\n",
      "Labels with 1:3899\n"
     ]
    }
   ],
   "source": [
    "count_open = len(train[train['LABEL']==0])\n",
    "count_closed = len(train[train['LABEL']==1])\n",
    "print(\"Labels with 0:\" + str(count_open))\n",
    "print(\"Labels with 1:\"+str(count_closed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a319561b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of open is 99.00471985888923\n"
     ]
    }
   ],
   "source": [
    "#Find the percentage of 0 in the label column of the training dataset\n",
    "count_open = len(train[train['LABEL']==0])\n",
    "count_closed = len(train[train['LABEL']==1])\n",
    "pct_of_open = count_open/(count_open +count_closed)\n",
    "print(\"percentage of open is\", pct_of_open*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3b7038",
   "metadata": {},
   "source": [
    "From analysis of the value counts of the 'Label' column from the train.csv, roughly 99% of the all entries are open, such that the value is '0'. This is indicative that the data is imbalanced, which is not ideal as most machine learning algorithms work better when the number of samples in each class are roughly equal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7ce8db",
   "metadata": {},
   "source": [
    "The next step would be to clean the data, particularily, if there is any missing values, which there is none. The test.csv does have all its 'Label' values labelled as 'NaN', which is because they need to filled in with the model. All the columns of the dataframe are also determined to be numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ea381b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "#Find number of columns in train that have missing values, which is none of them\n",
    "missing_columns=train.isnull().sum()\n",
    "print(missing_columns[missing_columns>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c8b8473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL    20000\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_columns=test.isnull().sum()\n",
    "print(missing_columns[missing_columns>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "58093e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID           int64\n",
       "USERID       int64\n",
       "EVENTID      int64\n",
       "TIMESTAMP    int64\n",
       "LABEL        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#All the columns of the training set are numerical\n",
    "train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625c09da",
   "metadata": {},
   "source": [
    "To balance the data more, up-sampling the minority class is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "61d87348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    387850\n",
       "1    387850\n",
       "Name: LABEL, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate majority and minority classes\n",
    "df_majority = train[train.LABEL==0]\n",
    "df_minority = train[train.LABEL==1]\n",
    " \n",
    "# Upsample minority\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,    \n",
    "                                 n_samples=387850,    # equals the majority class\n",
    "                                 random_state=120) # make a seed to reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    " \n",
    "# Display new class counts\n",
    "df_upsampled.LABEL.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6950395",
   "metadata": {},
   "source": [
    "For the training data, drop the target column to separate the target from the other features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5628a31",
   "metadata": {},
   "source": [
    "Note that the 'ID' columns are not features, but rather identifiers. The target column, 'LABEL' should also be removed and labelled for the y column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b57f120",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the 'Label' column in the training set, X, and leave only the 'Label' column for the target, y\n",
    "y = df_upsampled.LABEL\n",
    "X=df_upsampled.drop(['LABEL'], axis=1)\n",
    "test.drop(['LABEL'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2bfbc1",
   "metadata": {},
   "source": [
    "After ensuring the best features are chosen, implement the logistic regression model. Logistic regression is chosen as the result will be binary classification. Other machine learning algorithms that could have been chosen is k-nearest neighbours, decisions trees, and SVM (support vector machine)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a007b02e",
   "metadata": {},
   "source": [
    "The next step would be to split the dataset into training and validation sets with X_train and y_train being the training set features and target, and X_valid and y_valid are the validatin set features and target. This is to prevent the model from overfitting and to evaluate the model. A porportion of 80% of the dataset for the training set is chosen and 20% of the validation set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266b1499",
   "metadata": {},
   "source": [
    "The model is then defined and fitted, such that the model is trained on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de34c937",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split and fit the X,y data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "616e31e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg =  LogisticRegression()\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16808e5",
   "metadata": {},
   "source": [
    "At this stage, analyze the features and find some statistic results to evluate the best features. All the p-values of the features are < 0.05, hence being good features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "acc59698",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.669142\n",
      "         Iterations 4\n",
      "                          Results: Logit\n",
      "===================================================================\n",
      "Model:              Logit            Pseudo R-squared: 0.035       \n",
      "Dependent Variable: LABEL            AIC:              1038114.6070\n",
      "Date:               2021-09-27 23:12 BIC:              1038160.8531\n",
      "No. Observations:   775700           Log-Likelihood:   -5.1905e+05 \n",
      "Df Model:           3                LL-Null:          -5.3767e+05 \n",
      "Df Residuals:       775696           LLR p-value:      0.0000      \n",
      "Converged:          1.0000           Scale:            1.0000      \n",
      "No. Iterations:     4.0000                                         \n",
      "--------------------------------------------------------------------\n",
      "                Coef.   Std.Err.     z      P>|z|    [0.025   0.975]\n",
      "--------------------------------------------------------------------\n",
      "ID              0.0000    0.0000   55.7741  0.0000   0.0000   0.0000\n",
      "USERID          0.0002    0.0000  156.4489  0.0000   0.0002   0.0002\n",
      "EVENTID        -0.0102    0.0002  -67.8205  0.0000  -0.0105  -0.0099\n",
      "TIMESTAMP      -0.0000    0.0000  -61.1369  0.0000  -0.0000  -0.0000\n",
      "===================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Get stats on the features\n",
    "import statsmodels.api as sm\n",
    "logit_model=sm.Logit(y,X)\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f3a279b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.58\n"
     ]
    }
   ],
   "source": [
    "#Use the validation set to measure the accuracy of the classifier\n",
    "y_pred = logreg.predict(X_test)\n",
    "score_valid=mean_absolute_error(y_test,y_pred)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641af7a4",
   "metadata": {},
   "source": [
    "The test predictions are then generated and created into a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ff45a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the model to predict on the test .csv file\n",
    "y_pred = logreg.predict(test)\n",
    "submission=pd.DataFrame({'ID':test.index+391749,'LABEL':y_pred})\n",
    "submission.to_csv('result.csv',index=False)\n",
    "df3 = pd.merge(test, submission, on = 'ID')\n",
    "df3.set_index('ID', inplace = True)\n",
    "df3.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "51410a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    12558\n",
       "1     7442\n",
       "Name: LABEL, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count the number of 0 and 1 in the result csv file\n",
    "submission=pd.read_csv('result.csv')\n",
    "submission['LABEL'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3cc57f",
   "metadata": {},
   "source": [
    "This is not very plausible as only roughly 1% of the values are '1' for the 'LABEL' column. Next, a confusion matrix can then be created to evaluate the performance of the classification model with the left rows being the predicted values, positive and negative from top to down, respectively and the top row being the actual value, being positive to negative from left to right, respectively [1]. There are 6290+3737=10,027 correct predictions and 6268+3705=9973 incorrect predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f3a36164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6290 3705]\n",
      " [6268 3737]]\n"
     ]
    }
   ],
   "source": [
    "#Create the confusion matrix\n",
    "y_test=y_test[:20000]\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "62a831c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.63      0.56      9995\n",
      "           1       0.50      0.37      0.43     10005\n",
      "\n",
      "    accuracy                           0.50     20000\n",
      "   macro avg       0.50      0.50      0.49     20000\n",
      "weighted avg       0.50      0.50      0.49     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667039eb",
   "metadata": {},
   "source": [
    "The precision is true_positive/(true_positive+false_positive) and indicates the the ability for the model to not label a negative sample as positive. Another important metric is ratio, which is (true_positive)/(true_positive+false_negative), which is the ability for the classifier to find all the positive samples. Both of these are important to consider to evaluate the overall model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f780d4",
   "metadata": {},
   "source": [
    "We should use another machine learning algorithm, such as KNN or Random Forest Classifier algorithm, instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065534ad",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f68f48",
   "metadata": {},
   "source": [
    "If I had more information about each event and customer, I would use that to group the customer types together and events to do more feature analysis and feature engineering. Some event types might result in a higher correlation with a customer closing their account, such as a customer interacting with a website. I could also create new features from the new information, such as if a certain user and event can produce more important information. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7bce70",
   "metadata": {},
   "source": [
    "If I had more time, I would also consider other algorithms other than logistical regression and KNN, such as decision trees, and support vector machine (SVM), which is also are effective for binary classification. I would also use consider other ways to balance the data for the model, such as down-scaling the majoriy class, and tree-based algorithms, such as the Random Forest Classifier [2]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b246f5f5",
   "metadata": {},
   "source": [
    "Another method to improve my analysis includes tuning the machine learning algorithm used, such as for k-nearest neighbours, altering the k-value [3]. Other methods include bagging, which is to implement small sample populations and take a mean of the predictions, and boosting, which is to adust the weight of an observation based on the results of the previous classification, thereby strengthening predictive models [4]. This reduces the overall bias error of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f6f4ba",
   "metadata": {},
   "source": [
    "# Citations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6649314",
   "metadata": {},
   "source": [
    "[1] S. Li, “Building a logistic regression in Python, step by step,” Medium, 27-Feb-2019. [Online]. Available: https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8. [Accessed: 27-Sep-2021]. \n",
    "\n",
    "[2] “How to handle imbalanced classes in machine learning,” EliteDataScience, 23-May-2020. [Online]. Available: https://elitedatascience.com/imbalanced-classes. [Accessed: 26-Sep-2021]. \n",
    "\n",
    "[3] S. Ray, “How to increase accuracy of machine learning model,” Analytics Vidhya, 26-Jun-2020. [Online]. Available: https://www.analyticsvidhya.com/blog/2015/12/improve-machine-learning-results/. [Accessed: 26-Sep-2021]. \n",
    "\n",
    "[4] T. Srivastava, “Ensemble learning: Ensemble learning techniques,” Analytics Vidhya, 26-Jun-2020. [Online]. Available: https://www.analyticsvidhya.com/blog/2015/08/introduction-ensemble-learning/. [Accessed: 26-Sep-2021]. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
